---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: sabnzbd
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.5.1
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  driftDetection:
    mode: enabled
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  values:
    controllers:
      sabnzbd:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          sabnzbd:
            image:
              repository: ghcr.io/jfroy/sabnzbd
              tag: 4.3.3@sha256:93e1460dd1631b464d4ec05cc11a1ade189510a05d5da65c808ef4b9eb72f428
            env:
              SABNZBD__HOST_WHITELIST_ENTRIES: >-
                sabnzbd,
                sabnzbd.default,
                sabnzbd.default.svc,
                sabnzbd.default.svc.cluster,
                sabnzbd.default.svc.cluster.local,
                ${APP_SUBDOMAIN:-${APP}}.${PUBLIC_DOMAIN0},
                ${APP_SUBDOMAIN:-${APP}}.${TAILNET_DOMAIN}
              SABNZBD__PORT: &port 80
              TZ: America/Los_Angeles
            envFrom:
              - secretRef:
                  name: sabnzbd-secret
            probes:
              liveness: &probe
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /api?mode=version
                    port: *port
              readiness: *probe
            securityContext: &securityContext
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }
            resources:
              requests:
                cpu: 1
                memory: 2Gi
              limits:
                cpu: 4
                memory: 50Gi
        initContainers:
          gluetun:
            image:
              repository: ghcr.io/jfroy/gluetun
              tag: v3.39.0-jfroy.3@sha256:01466321c40563cf95e9d57ec978802072c4d51c8665646d3c80a72b25fe0dde
            env:
              BLOCK_MALICIOUS: "off" # save 300MB of RAM; https://github.com/qdm12/gluetun/issues/2054
              DOT_IPV6: "on"
              FIREWALL_DEBUG: on
              FIREWALL_INPUT_PORTS: "80,9999"
              HEALTH_SERVER_ADDRESS: ":9999"
              HEALTH_VPN_DURATION_INITIAL: 60s
              LOG_LEVEL: debug
              STORAGE_FILEPATH: "" # prevent memory spike and avoid I/O
              VERSION_INFORMATION: off
              VPN_INTERFACE: wg0
              VPN_TYPE: wireguard
              TZ: America/Los_Angeles
            envFrom:
              - secretRef:
                  name: sabnzbd-gluetun-secret
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /
                    port: 9999
              startup:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /
                    port: 9999
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  failureThreshold: 5
            resources:
              requests:
                memory: 48Mi
              limits:
                memory: 96Mi
                kernel.org/tun: 1
            restartPolicy: Always
            securityContext:
              <<: *securityContext
              readOnlyRootFilesystem: false
              runAsNonRoot: false
              runAsUser: 0
              capabilities: { add: ["NET_ADMIN", "NET_RAW"] }
        pod:
          labels:
            gluetun: "true"
          hostname: sabnzbd
    defaultPodOptions:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch
        supplementalGroups: [4000]
        seccompProfile: { type: RuntimeDefault }
    service:
      sabnzbd:
        controller: sabnzbd
        ports:
          http:
            port: *port
    ingress:
      sabnzbd:
        className: internal
        annotations:
          nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${PUBLIC_DOMAIN0}/oauth2/auth
          nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${PUBLIC_DOMAIN0}/oauth2/start?rd=$scheme://$best_http_host$request_uri
          nginx.ingress.kubernetes.io/auth-response-headers: "x-auth-request-email"
        hosts:
          - host: "${APP_SUBDOMAIN:-${APP}}.${PUBLIC_DOMAIN0}"
            paths:
              - path: /
                service:
                  identifier: sabnzbd
                  port: http
    persistence:
      config:
        existingClaim: ${APP}
        advancedMounts:
          sabnzbd:
            sabnzbd:
              - path: /config
      empty:
        type: emptyDir
        sizeLimit: 20Mi
        globalMounts:
          - path: /gluetun
            subPath: gluetun
          - path: /tmp
            subPath: tmp
      media2:
        type: persistentVolumeClaim
        existingClaim: default-media-smb-kantai1
        advancedMounts:
          sabnzbd:
            sabnzbd:
              - path: /media2/sabnzbd
                subPath: sabnzbd
      logs:
        type: emptyDir
        advancedMounts:
          sabnzbd:
            sabnzbd:
              - path: /config/logs
      run:
        type: emptyDir
        medium: Memory
        sizeLimit: 10Mi
        globalMounts:
          - path: /run
          - path: /var/run
