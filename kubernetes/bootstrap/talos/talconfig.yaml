# yaml-language-server: $schema=https://raw.githubusercontent.com/budimanjojo/talhelper/master/pkg/config/schemas/talconfig.json
---
# renovate: datasource=docker depName=ghcr.io/jfroy/siderolabs/imager
talosVersion: v1.8.3000
# renovate: datasource=docker depName=ghcr.io/siderolabs/kubelet
kubernetesVersion: v1.31.4

clusterName: "kantai"
endpoint: https://192.168.1.8:6443
clusterPodNets:
  - "10.11.0.0/16"
  - "${CLUSTER_POD_V6_CIDR}"
clusterSvcNets:
  - "10.12.0.0/16"
  - "${CLUSTER_SVC_V6_CIDR}"
additionalApiServerCertSans: &sans
  - "192.168.1.8"
  - "127.0.0.1" # KubePrism
additionalMachineCertSans: *sans

# Disable built-in Flannel to use Cilium
cniConfig:
  name: none

imageFactory:
  registryURL: tif.etincelle.cloud

nodes:
  - hostname: "kantai1"
    ipAddress: "192.168.1.13"
    installDiskSelector:
      wwid: eui.e8238fa6bf530001001b448b4a49b465
    controlPlane: true
    kernelModules:
      - name: nvidia
      - name: nvidia-uvm
      - name: nvidia-modeset
      - name: nvme-tcp
      - name: zfs
    machineSpec:
      secureboot: true
      useUKI: true
    networkInterfaces:
      - interface: enp129s0f0
        dhcp: true
        mtu: 1500
        vip:
          ip: "192.168.1.8"
      - interface: enp129s0f1
        dhcp: true
        mtu: 1500
    schematic:
      customization:
        extraKernelArgs:
          - lsm=landlock,lockdown,yama,apparmor,bpf
          - lsm.debug
          - amd_pstate=active
          - spec_rstack_overflow=microcode
          - zfs.zfs_arc_max=26843545600
          - zfs.zfs_arc_shrinker_limit=0
        systemExtensions:
          officialExtensions:
            - jfroy/siderolabs/nvidia-driver-production
            - jfroy/siderolabs/zfs
            - siderolabs/amd-ucode
            - siderolabs/glibc
    patches:
      # sysctls
      - |-
        machine:
          sysctls:
            net.core.bpf_jit_harden: "1"
            net.ipv6.conf.all.accept_ra: "2"
            net.ipv6.conf.all.forwarding: "1"
            net.ipv6.conf.enp129s0f0np0.accept_ra: "2"
            net.ipv6.conf.enp129s0f0np0.forwarding: "1"
            net.ipv6.conf.enp129s0f1np1.accept_ra: "2"
            net.ipv6.conf.enp129s0f1np1.forwarding: "1"
            vm.nr_hugepages: "1024"

      # nvidia containerd runtimes
      - |-
        machine:
          files:
            - op: create
              path: /etc/cri/conf.d/20-customization.part
              content: |-
                [plugins.'io.containerd.cri.v1.images']
                  discard_unpacked_layers = false
                [plugins.'io.containerd.cri.v1.runtime']
                  enable_unprivileged_ports = true
                  enable_unprivileged_icmp = true
                [plugins.'io.containerd.cri.v1.runtime'.containerd]
                  default_runtime_name = "runc"
                [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia]
                  container_annotations = ["nvidia.cdi.k8s.io/*"]
                  privileged_without_host_devices = false
                  runtime_engine = ""
                  runtime_root = ""
                  runtime_type = "io.containerd.runc.v2"
                  [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia.options]
                    BinaryName = "/var/nvidia/toolkit/nvidia-container-runtime"
                [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-cdi]
                  container_annotations = ["nvidia.cdi.k8s.io/*"]
                  privileged_without_host_devices = false
                  runtime_engine = ""
                  runtime_root = ""
                  runtime_type = "io.containerd.runc.v2"
                  [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-cdi.options]
                    BinaryName = "/var/nvidia/toolkit/nvidia-container-runtime.cdi"
                [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-legacy]
                  container_annotations = ["nvidia.cdi.k8s.io/*"]
                  privileged_without_host_devices = false
                  runtime_engine = ""
                  runtime_root = ""
                  runtime_type = "io.containerd.runc.v2"
                  [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-legacy.options]
                    BinaryName = "/var/nvidia/toolkit/nvidia-container-runtime.legacy"

      # Encrypt system disk with TPM
      - |-
        machine:
          systemDiskEncryption:
            ephemeral:
              provider: luks2
              keys:
                - slot: 0
                  tpm: {}
            state:
              provider: luks2
              keys:
                - slot: 0
                  tpm: {}

      # udev rules
      # - set all disks to no scheduler (optimal setting for Ceph and ZFS)
      - |-
        machine:
          udev:
            rules:
              # set all disks to `none` scheduler (optimal setting for Ceph and ZFS)
              - SUBSYSTEM=="block", ENV{DEVTYPE}=="disk", ATTR{queue/scheduler}="none"

  - hostname: "kantai2"
    ipAddress: "192.168.1.15"
    installDiskSelector:
      size: "> 50GB"
    controlPlane: true
    networkInterfaces:
      - interface: enxf243cb7739d7
        dhcp: true
        mtu: 1500
        vip:
          ip: "192.168.1.8"
    schematic:
      customization:
        extraKernelArgs:
          - lsm=landlock,lockdown,yama,apparmor,bpf
          - lsm.debug
    nodeTaints:
      node-role.kubernetes.io/control-plane: :NoSchedule
    patches:
      # sysctls
      - |-
        machine:
          sysctls:
            net.core.bpf_jit_harden: "1"
            net.ipv6.conf.all.accept_ra: "2"
            net.ipv6.conf.all.forwarding: "1"
            net.ipv6.conf.enp0s1.accept_ra: "2"
            net.ipv6.conf.enp0s1.forwarding: "1"

  - hostname: "kantai3"
    ipAddress: "192.168.1.10"
    installDiskSelector:
      wwid: eui.e8238fa6bf530001001b448b4e89de82
    controlPlane: true
    kernelModules:
      - name: nvidia
      - name: nvidia-uvm
      - name: nvidia-modeset
      - name: zfs
    networkInterfaces:
      - interface: enp1s0
        dhcp: true
        mtu: 1500
        vip:
          ip: "192.168.1.8"
    nodeTaints:
      node-role.kubernetes.io/control-plane: :NoSchedule
    schematic:
      customization:
        extraKernelArgs:
          - lsm=landlock,lockdown,yama,apparmor,bpf
          - lsm.debug
          - amd_pstate=active
          - spec_rstack_overflow=microcode
          - zfs.zfs_arc_max=6871947674
          - zfs.zfs_arc_shrinker_limit=0
        systemExtensions:
          officialExtensions:
            - jfroy/siderolabs/nvidia-driver-production
            - jfroy/siderolabs/zfs
            - siderolabs/amd-ucode
            - siderolabs/glibc
    patches:
      # sysctls
      - |-
        machine:
          sysctls:
            net.core.bpf_jit_harden: "1"
            net.ipv6.conf.all.accept_ra: "2"
            net.ipv6.conf.all.forwarding: "1"
            net.ipv6.conf.enp1s0.accept_ra: "2"
            net.ipv6.conf.enp1s0.forwarding: "1"
            vm.nr_hugepages: "1024"

      # udev rules
      # - set all disks to no scheduler (optimal setting for Ceph and ZFS)
      - |-
        machine:
          udev:
            rules:
              # set all disks to `none` scheduler (optimal setting for Ceph and ZFS)
              - SUBSYSTEM=="block", ENV{DEVTYPE}=="disk", ATTR{queue/scheduler}="none"

patches:
  # Disable search domain everywhere
  - |-
    machine:
      network:
        disableSearchDomain: true

  # Enable etcd discovery and disable external service discovery.
  # https://www.talos.dev/v1.8/talos-guides/discovery/#discovery-service
  - |-
    cluster:
      discovery:
        enabled: true
        registries:
          kubernetes:
            disabled: false
          service:
            disabled: true

  # Configure kubelet
  - |-
    machine:
      kubelet:
        extraArgs:
          rotate-server-certificates: true
        extraConfig:
          maxPods: 400
        nodeIP:
          validSubnets:
            - 192.168.1.0/27
            - ${CLUSTER_NODE_V6_CIDR}

  # Custom sysctl settings
  - |-
    machine:
      sysctls:
        fs.inotify.max_queued_events: "65536"
        fs.inotify.max_user_watches: "1048576"  # Watchdog
        fs.inotify.max_user_instances: "8192"   # Watchdog
        net.core.default_qdisc: fq_codel        # Fair Queue CoDel packet scheduler to fight bufferbloat
        net.core.rmem_max: "67108864"           # 64 MiB; QUIC and 10 Gbps
        net.core.wmem_max: "67108864"           # 64 MiB; QUIC and 10 Gbps
        net.ipv4.tcp_rmem: 4096 131072 33554432  # 1K, 128K, 32M; QUIC and 10 Gbps
        net.ipv4.tcp_wmem: 4096 131072 33554432  # 1K, 128K, 32M; QUIC and 10 Gbps

  # Mount openebs-hostpath in kubelet
  - |-
    machine:
      kubelet:
        extraMounts:
          - destination: /var/openebs/local
            type: bind
            source: /var/openebs/local
            options:
              - bind
              - rshared
              - rw

  # Enable Host DNS: https://www.talos.dev/v1.8/talos-guides/network/host-dns/
  - |-
    machine:
      features:
        hostDNS:
          enabled: true
          resolveMemberNames: true
          # Incompatible with Cilium bpf masquerade. https://github.com/siderolabs/talos/issues/8836
          forwardKubeDNSToHost: false

  # NFS mount options
  - &nfsPatch |
    machine:
      files:
        - op: overwrite
          path: /etc/nfsmount.conf
          permissions: 0o644
          content: |
            [ NFSMount_Global_Options ]
            hard=True
            noatime=True
            nodiratime=True

controlPlane:
  patches:
    # Cluster configuration
    - |-
      cluster:
        allowSchedulingOnControlPlanes: true
        controllerManager:
          extraArgs:
            bind-address: 0.0.0.0
            node-cidr-mask-size-ipv6: "118"
        coreDNS:
          disabled: true
        proxy:
          disabled: true
        scheduler:
          extraArgs:
            bind-address: 0.0.0.0

    # ETCD configuration
    - |-
      cluster:
        etcd:
          extraArgs:
            listen-metrics-urls: http://0.0.0.0:2381
          advertisedSubnets:
            - 192.168.1.0/27

    # api-server configuration
    - |-
      cluster:
        apiServer:
          extraArgs:
            oidc-client-id: "${OIDC_CLIENT_ID}"
            oidc-issuer-url: "${OIDC_ISSUER_URL}"
            oidc-username-claim: email

    # Disable default API server admission plugins.
    - |-
      - op: remove
        path: /cluster/apiServer/admissionControl

    # Enable K8s Talos API Access
    - |-
      machine:
        features:
          kubernetesTalosAPIAccess:
            enabled: true
            allowedRoles:
              - os:admin
              - os:etcd:backup
            allowedKubernetesNamespaces:
              - talos-admin

    # Default scheduler topology spread constraints
    - |-
      cluster:
        scheduler:
          config:
            apiVersion: kubescheduler.config.k8s.io/v1
            kind: KubeSchedulerConfiguration
            profiles:
              - schedulerName: default-scheduler
                pluginConfig:
                  - name: PodTopologySpread
                    args:
                      defaultingType: List
                      defaultConstraints:
                        - maxSkew: 1
                          topologyKey: "kubernetes.io/hostname"
                          whenUnsatisfiable: ScheduleAnyway
                        - maxSkew: 3
                          topologyKey: "topology.kubernetes.io/zone"
                          whenUnsatisfiable: ScheduleAnyway
