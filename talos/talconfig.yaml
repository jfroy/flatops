# yaml-language-server: $schema=https://raw.githubusercontent.com/budimanjojo/talhelper/master/pkg/config/schemas/talconfig.json
---
# renovate: datasource=docker depName=ghcr.io/jfroy/siderolabs/imager
talosVersion: v1.11.1
# renovate: datasource=docker depName=ghcr.io/siderolabs/kubelet
kubernetesVersion: v1.33.5

clusterName: "kantai"
endpoint: https://k8s.kantai.xyz:6443
clusterPodNets:
  - "10.11.0.0/16"
  - "${CLUSTER_POD_V6_CIDR}"
clusterSvcNets:
  - "10.12.0.0/16"
  - "${CLUSTER_SVC_V6_CIDR}"
additionalApiServerCertSans: &sans
  - "k8s.kantai.xyz"
  - "10.10.10.10"
  - "127.0.0.1" # KubePrism
additionalMachineCertSans: *sans

# Disable built-in Flannel to use Cilium
cniConfig:
  name: none

imageFactory:
  registryURL: tif.etincelle.cloud

nodes:
  - hostname: "kantai1"
    ipAddress: "10.1.1.1"
    installDiskSelector:
      wwid: eui.e8238fa6bf530001001b448b4a49b465
    controlPlane: true
    kernelModules:
      - name: nvidia
      - name: nvidia-uvm
      - name: nvidia-modeset
      - name: nvme-tcp
      - name: vfio-pci
    machineSpec:
      secureboot: true
      useUKI: true
    networkInterfaces:
      - interface: enp129s0f0np0
        dhcp: true
        mtu: 1500
      - interface: enp129s0f1np1
        ignore: true
      - interface: enp66s0f0
        ignore: true
      - interface: enp66s0f1
        ignore: true
    nodeLabels:
      openebs.io/engine: mayastor
    schematic:
      customization:
        extraKernelArgs:
          - -selinux
          - lsm=landlock,lockdown,yama,apparmor,bpf
          - lsm.debug
          - amd_pstate=active
          - spec_rstack_overflow=microcode
          - zfs.zfs_arc_max=26843545600
          - zfs.zfs_arc_shrinker_limit=0
        systemExtensions:
          officialExtensions:
            - jfroy/siderolabs/glibc
            - jfroy/siderolabs/nvidia-driver-production
            - jfroy/siderolabs/zfs
            - siderolabs/amd-ucode
            - siderolabs/iscsi-tools
            - siderolabs/util-linux-tools
    patches:
      # sysctls
      - |-
        machine:
          sysctls:
            net.core.bpf_jit_harden: "1"
            net.ipv6.conf.all.accept_ra: "2"
            net.ipv6.conf.all.forwarding: "1"
            net.ipv6.conf.enp129s0f0np0.accept_ra: "2"
            net.ipv6.conf.enp129s0f0np0.forwarding: "1"
            net.ipv6.conf.enp129s0f1np1.accept_ra: "2"
            net.ipv6.conf.enp129s0f1np1.forwarding: "1"
            vm.nr_hugepages: "2048"

      # Encrypt EPHEMERAL and STATE volumes
      - |-
        machine:
          systemDiskEncryption:
            ephemeral:
              provider: luks2
              keys:
                - slot: 0
                  tpm:
                    checkSecurebootStatusOnEnroll: true
              options:
                - no_read_workqueue
                - no_write_workqueue
            state:
              provider: luks2
              keys:
                - slot: 0
                  tpm:
                    checkSecurebootStatusOnEnroll: true
              options:
                - no_read_workqueue
                - no_write_workqueue

      # udev rules
      # - set all disks to no scheduler (optimal setting for Ceph and ZFS)
      - |-
        machine:
          udev:
            rules:
              # set all disks to `none` scheduler (optimal setting for Ceph and ZFS)
              - SUBSYSTEM=="block", ENV{DEVTYPE}=="disk", ATTR{queue/scheduler}="none"

  - hostname: "kantai2"
    ipAddress: "10.1.1.2"
    installDiskSelector:
      size: "> 50GB"
    controlPlane: true
    kernelModules:
      - name: nvme-tcp
    machineSpec:
      secureboot: true
      useUKI: true
    networkInterfaces:
      - interface: enp0s1
        dhcp: true
        mtu: 1500
    schematic:
      customization:
        extraKernelArgs:
          - -selinux
          - lsm=landlock,lockdown,yama,apparmor,bpf
          - lsm.debug
        systemExtensions:
          officialExtensions:
            - siderolabs/iscsi-tools
            - siderolabs/util-linux-tools
    nodeTaints:
      node-role.kubernetes.io/control-plane: :NoSchedule
    patches:
      # sysctls
      - |-
        machine:
          sysctls:
            net.core.bpf_jit_harden: "1"
            net.ipv6.conf.all.accept_ra: "2"
            net.ipv6.conf.all.forwarding: "1"
            net.ipv6.conf.enp0s1.accept_ra: "2"
            net.ipv6.conf.enp0s1.forwarding: "1"
            vm.nr_hugepages: "1024"

  - hostname: "kantai3"
    ipAddress: "10.1.1.3"
    installDiskSelector:
      wwid: eui.0025385691b1844a
    controlPlane: true
    kernelModules:
      - name: nvidia
      - name: nvidia-uvm
      - name: nvidia-modeset
      - name: nvme-tcp
    machineSpec:
      secureboot: false
      useUKI: true
    networkInterfaces:
      - interface: enp1s0
        dhcp: true
        mtu: 1500
    nodeTaints:
      node-role.kubernetes.io/control-plane: :NoSchedule
    schematic:
      customization:
        extraKernelArgs:
          - -selinux
          - lsm=landlock,lockdown,yama,apparmor,bpf
          - lsm.debug
          - amd_pstate=active
          - spec_rstack_overflow=microcode
          - zfs.zfs_arc_max=6871947674
          - zfs.zfs_arc_shrinker_limit=0
        systemExtensions:
          officialExtensions:
            - jfroy/siderolabs/glibc
            - jfroy/siderolabs/nvidia-driver-production
            - jfroy/siderolabs/zfs
            - siderolabs/amd-ucode
            - siderolabs/iscsi-tools
            - siderolabs/util-linux-tools
    patches:
      # sysctls
      - |-
        machine:
          sysctls:
            net.core.bpf_jit_harden: "1"
            net.ipv6.conf.all.accept_ra: "2"
            net.ipv6.conf.all.forwarding: "1"
            net.ipv6.conf.enp1s0.accept_ra: "2"
            net.ipv6.conf.enp1s0.forwarding: "1"
            vm.nr_hugepages: "2048"

      # udev rules
      # - set all disks to no scheduler (optimal setting for Ceph and ZFS)
      - |-
        machine:
          udev:
            rules:
              # set all disks to `none` scheduler (optimal setting for Ceph and ZFS)
              - SUBSYSTEM=="block", ENV{DEVTYPE}=="disk", ATTR{queue/scheduler}="none"

patches:
  # Disable search domain everywhere
  - |-
    machine:
      network:
        disableSearchDomain: true

  # Use etincelle discovery-service.
  # The previous Kubernetes discovery service is deprecated due to security changes in Kubernetes.
  # https://www.talos.dev/v1.10/talos-guides/discovery/#registries
  # https://github.com/siderolabs/talos/issues/9980
  - |-
    cluster:
      discovery:
        enabled: true
        registries:
          kubernetes:
            disabled: true
          service:
            disabled: false
            endpoint: https://ds.etincelle.cloud/

  # Configure kubelet
  - |-
    machine:
      kubelet:
        defaultRuntimeSeccompProfileEnabled: true
        extraConfig:
          featureGates:
            UserNamespacesSupport: true
            UserNamespacesPodSecurityStandards: true
          maxParallelImagePulls: 10
          maxPods: 400
          serializeImagePulls: false
          serverTLSBootstrap: true
        nodeIP:
          validSubnets:
            - 10.1.1.0/24
            - ${CLUSTER_NODE_V6_CIDR}

  # Custom sysctl settings
  - |-
    machine:
      sysctls:
        fs.inotify.max_queued_events: "65536"
        fs.inotify.max_user_watches: "1048576"
        fs.inotify.max_user_instances: "8192"
        net.core.default_qdisc: fq_codel           # Fair Queue CoDel packet scheduler to fight bufferbloat
        net.core.rmem_max: "67108864"              # 64 MiB; QUIC and 10 Gbps
        net.core.wmem_max: "67108864"              # 64 MiB; QUIC and 10 Gbps
        net.ipv4.ping_group_range: "0 2147483647"  # Allow all groups to ping
        net.ipv4.tcp_rmem: 4096 131072 33554432    # 1K, 128K, 32M; QUIC and 10 Gbps
        net.ipv4.tcp_wmem: 4096 131072 33554432    # 1K, 128K, 32M; QUIC and 10 Gbps
        user.max_user_namespaces: "49152"          # C000; arbitrary

  # Extra mounts for openebs-hostpath
  - |-
    machine:
      kubelet:
        extraMounts:
          - destination: /var/openebs/local
            type: bind
            source: /var/openebs/local
            options:
              - bind
              - rshared
              - rw

  # Enable Host DNS: https://www.talos.dev/v1.8/talos-guides/network/host-dns/
  - |-
    machine:
      features:
        hostDNS:
          enabled: true
          resolveMemberNames: true
          # Incompatible with Cilium bpf masquerade. https://github.com/siderolabs/talos/issues/8836
          forwardKubeDNSToHost: false

  # Use GCR mirror for Docker Hub
  # NOTE: Incompatible with Spegel, remove once Spegel can be used again.
  - |-
    machine:
      registries:
        mirrors:
          docker.io:
            endpoints:
              - https://mirror.gcr.io

  # containerd config
  # - discard_unpacked_layers (spegel, https://spegel.dev/docs/getting-started/#talos)
  # - enable_unprivileged_ports, enable_unprivileged_icmp (convenience, https://github.com/kubernetes/kubernetes/issues/102612)
  # - cgroup_writable (buildkit, https://github.com/containerd/containerd/pull/11131, planned for containerd 2.1)
  # - nvidia runtimes (nvidia, https://www.talos.dev/latest/talos-guides/configuration/nvidia-gpu/)
  - |-
    machine:
      files:
        - op: create
          path: /etc/cri/conf.d/20-customization.part
          content: |-
            [plugins.'io.containerd.cri.v1.images']
              discard_unpacked_layers = false
            [plugins.'io.containerd.cri.v1.runtime']
              enable_unprivileged_ports = true
              enable_unprivileged_icmp = true
            [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.runc]
              cgroup_writable = true
            [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia]
              container_annotations = ["nvidia.cdi.k8s.io/*"]
              privileged_without_host_devices = false
              runtime_engine = ""
              runtime_root = ""
              runtime_type = "io.containerd.runc.v2"
              [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia.options]
                BinaryName = "/var/nvidia/toolkit/nvidia-container-runtime"
            [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-cdi]
              container_annotations = ["nvidia.cdi.k8s.io/*"]
              privileged_without_host_devices = false
              runtime_engine = ""
              runtime_root = ""
              runtime_type = "io.containerd.runc.v2"
              [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-cdi.options]
                BinaryName = "/var/nvidia/toolkit/nvidia-container-runtime.cdi"
            [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-legacy]
              container_annotations = ["nvidia.cdi.k8s.io/*"]
              privileged_without_host_devices = false
              runtime_engine = ""
              runtime_root = ""
              runtime_type = "io.containerd.runc.v2"
              [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-legacy.options]
                BinaryName = "/var/nvidia/toolkit/nvidia-container-runtime.legacy"

controlPlane:
  patches:
    # Cluster configuration
    - |-
      cluster:
        allowSchedulingOnControlPlanes: true
        controllerManager:
          extraArgs:
            bind-address: 0.0.0.0
            node-cidr-mask-size-ipv6: "108"
        coreDNS:
          disabled: true
        proxy:
          disabled: true
        scheduler:
          extraArgs:
            bind-address: 0.0.0.0

    # ETCD configuration
    - |-
      cluster:
        etcd:
          extraArgs:
            listen-metrics-urls: http://0.0.0.0:2381
          advertisedSubnets:
            - 10.1.1.0/24

    # api-server configuration
    - |-
      cluster:
        apiServer:
          extraArgs:
            disable-admission-plugins: PodSecurity
            enable-aggregator-routing: true
            feature-gates: UserNamespacesSupport=true,UserNamespacesPodSecurityStandards=true
            oidc-client-id: "${OIDC_CLIENT_ID}"
            oidc-issuer-url: "${OIDC_ISSUER_URL}"
            oidc-username-claim: email

    # Enable K8s Talos API Access
    - |-
      machine:
        features:
          kubernetesTalosAPIAccess:
            enabled: true
            allowedRoles:
              - os:admin
              - os:etcd:backup
            allowedKubernetesNamespaces:
              - talos-admin
