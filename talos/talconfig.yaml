# yaml-language-server: $schema=https://raw.githubusercontent.com/budimanjojo/talhelper/master/pkg/config/schemas/talconfig.json
---
# renovate: datasource=docker depName=ghcr.io/jfroy/siderolabs/imager
talosVersion: v1.9.14005
# renovate: datasource=docker depName=ghcr.io/siderolabs/kubelet
kubernetesVersion: v1.32.1

clusterName: "kantai"
endpoint: https://10.1.1.0:6443
clusterPodNets:
  - "10.11.0.0/16"
clusterSvcNets:
  - "10.12.0.0/16"
additionalApiServerCertSans: &sans
  - "10.1.1.0"
  - "127.0.0.1" # KubePrism
additionalMachineCertSans: *sans

# Disable built-in Flannel to use Cilium
cniConfig:
  name: none

imageFactory:
  registryURL: tif.etincelle.cloud

nodes:
  - hostname: "kantai1"
    ipAddress: "10.1.1.1"
    installDiskSelector:
      wwid: eui.e8238fa6bf530001001b448b4a49b465
    controlPlane: true
    kernelModules:
      - name: nvidia
      - name: nvidia-uvm
      - name: nvidia-modeset
      - name: nvme-tcp
      - name: vfio-pci
    machineSpec:
      secureboot: true
      useUKI: true
    networkInterfaces:
      - interface: enp129s0f0np0
        dhcp: true
        mtu: 1500
        vip:
          ip: "10.1.1.0"
      - interface: enp129s0f1np1
        ignore: true
      - interface: enp66s0f0
        ignore: true
      - interface: enp66s0f1
        ignore: true
    schematic:
      customization:
        extraKernelArgs:
          - lsm=landlock,lockdown,yama,apparmor,bpf
          - lsm.debug
          - amd_pstate=active
          - spec_rstack_overflow=microcode
          - zfs.zfs_arc_max=26843545600
          - zfs.zfs_arc_shrinker_limit=0
        systemExtensions:
          officialExtensions:
            - jfroy/siderolabs/nvidia-driver-production
            - jfroy/siderolabs/zfs
            - siderolabs/amd-ucode
            - siderolabs/glibc
            - siderolabs/iscsi-tools
            - siderolabs/util-linux-tools
    patches:
      # sysctls
      - |-
        machine:
          sysctls:
            net.core.bpf_jit_harden: "1"
            net.ipv6.conf.all.accept_ra: "0"
            net.ipv6.conf.all.forwarding: "0"
            vm.nr_hugepages: "2048"

      # Encrypt EPHEMERAL and STATE volumes
      - |-
        machine:
          systemDiskEncryption:
            ephemeral:
              provider: luks2
              keys:
                - slot: 0
                  tpm:
                    checkSecurebootStatusOnEnroll: true
              options:
                - no_read_workqueue
                - no_write_workqueue
            state:
              provider: luks2
              keys:
                - slot: 0
                  tpm:
                    checkSecurebootStatusOnEnroll: true
              options:
                - no_read_workqueue
                - no_write_workqueue

      # udev rules
      # - set all disks to no scheduler (optimal setting for Ceph and ZFS)
      - |-
        machine:
          udev:
            rules:
              # set all disks to `none` scheduler (optimal setting for Ceph and ZFS)
              - SUBSYSTEM=="block", ENV{DEVTYPE}=="disk", ATTR{queue/scheduler}="none"

  - hostname: "kantai2"
    ipAddress: "10.1.1.2"
    installDiskSelector:
      size: "> 50GB"
    controlPlane: true
    networkInterfaces:
      - interface: enxf243cb7739d7
        dhcp: true
        mtu: 1500
        vip:
          ip: "10.1.1.0"
    schematic:
      customization:
        extraKernelArgs:
          - lsm=landlock,lockdown,yama,apparmor,bpf
          - lsm.debug
        systemExtensions:
          officialExtensions:
            - siderolabs/iscsi-tools
            - siderolabs/util-linux-tools
    nodeTaints:
      node-role.kubernetes.io/control-plane: :NoSchedule
    patches:
      # sysctls
      - |-
        machine:
          sysctls:
            net.core.bpf_jit_harden: "1"
            net.ipv6.conf.all.accept_ra: "0"
            net.ipv6.conf.all.forwarding: "0"

  - hostname: "kantai3"
    ipAddress: "10.1.1.3"
    installDiskSelector:
      wwid: eui.e8238fa6bf530001001b448b4e89de82
    controlPlane: true
    kernelModules:
      - name: nvidia
      - name: nvidia-uvm
      - name: nvidia-modeset
    networkInterfaces:
      - interface: enp1s0
        dhcp: true
        mtu: 1500
        vip:
          ip: "10.1.1.0"
    nodeTaints:
      node-role.kubernetes.io/control-plane: :NoSchedule
    schematic:
      customization:
        extraKernelArgs:
          - lsm=landlock,lockdown,yama,apparmor,bpf
          - lsm.debug
          - amd_pstate=active
          - spec_rstack_overflow=microcode
          - zfs.zfs_arc_max=6871947674
          - zfs.zfs_arc_shrinker_limit=0
        systemExtensions:
          officialExtensions:
            - jfroy/siderolabs/nvidia-driver-production
            - jfroy/siderolabs/zfs
            - siderolabs/amd-ucode
            - siderolabs/glibc
            - siderolabs/iscsi-tools
            - siderolabs/util-linux-tools
    patches:
      # sysctls
      - |-
        machine:
          sysctls:
            net.core.bpf_jit_harden: "1"
            net.ipv6.conf.all.accept_ra: "0"
            net.ipv6.conf.all.forwarding: "0"
            vm.nr_hugepages: "2048"

      # udev rules
      # - set all disks to no scheduler (optimal setting for Ceph and ZFS)
      - |-
        machine:
          udev:
            rules:
              # set all disks to `none` scheduler (optimal setting for Ceph and ZFS)
              - SUBSYSTEM=="block", ENV{DEVTYPE}=="disk", ATTR{queue/scheduler}="none"

patches:
  # Disable search domain everywhere
  - |-
    machine:
      network:
        disableSearchDomain: true

  # Use etincelle discovery-service.
  # The previous Kubernetes discovery service is deprecated due to security changes in Kubernetes.
  # https://www.talos.dev/v1.10/talos-guides/discovery/#registries
  # https://github.com/siderolabs/talos/issues/9980
  - |-
    cluster:
      discovery:
        enabled: true
        registries:
          kubernetes:
            disabled: true
          service:
            disabled: false
            endpoint: https://ds.etincelle.cloud/

  # Configure kubelet
  - |-
    machine:
      kubelet:
        defaultRuntimeSeccompProfileEnabled: true
        extraConfig:
          featureGates:
            UserNamespacesSupport: true
            UserNamespacesPodSecurityStandards: true
          maxParallelImagePulls: 10
          maxPods: 400
          serializeImagePulls: false
          serverTLSBootstrap: true
        nodeIP:
          validSubnets:
            - 10.1.1.0/24

  # Custom sysctl settings
  - |-
    machine:
      sysctls:
        fs.inotify.max_queued_events: "65536"
        fs.inotify.max_user_watches: "1048576"
        fs.inotify.max_user_instances: "8192"
        net.core.default_qdisc: fq_codel         # Fair Queue CoDel packet scheduler to fight bufferbloat
        net.core.rmem_max: "67108864"            # 64 MiB; QUIC and 10 Gbps
        net.core.wmem_max: "67108864"            # 64 MiB; QUIC and 10 Gbps
        net.ipv4.tcp_rmem: 4096 131072 33554432  # 1K, 128K, 32M; QUIC and 10 Gbps
        net.ipv4.tcp_wmem: 4096 131072 33554432  # 1K, 128K, 32M; QUIC and 10 Gbps
        user.max_user_namespaces: "49152"        # C000; arbitrary

  # Extra mounts for longhorn and openebs-hostpath
  - |-
    machine:
      kubelet:
        extraMounts:
          - destination: /var/lib/longhorn
            type: bind
            source: /var/lib/longhorn
            options:
              - bind
              - rshared
              - rw
          - destination: /var/openebs/local
            type: bind
            source: /var/openebs/local
            options:
              - bind
              - rshared
              - rw

  # Enable Host DNS: https://www.talos.dev/v1.8/talos-guides/network/host-dns/
  - |-
    machine:
      features:
        hostDNS:
          enabled: true
          resolveMemberNames: true
          # Incompatible with Cilium bpf masquerade. https://github.com/siderolabs/talos/issues/8836
          forwardKubeDNSToHost: false

  # containerd config
  # - discard_unpacked_layers (spegel, https://spegel.dev/docs/getting-started/#talos)
  # - enable_unprivileged_ports, enable_unprivileged_icmp (convenience, https://github.com/kubernetes/kubernetes/issues/102612)
  # - cgroup_writable (buildkit, https://github.com/containerd/containerd/pull/11131, planned for containerd 2.1)
  # - nvidia runtimes (nvidia, https://www.talos.dev/latest/talos-guides/configuration/nvidia-gpu/)
  - |-
    machine:
      files:
        - op: create
          path: /etc/cri/conf.d/20-customization.part
          content: |-
            [plugins.'io.containerd.cri.v1.images']
              discard_unpacked_layers = false
            [plugins.'io.containerd.cri.v1.runtime']
              enable_unprivileged_ports = true
              enable_unprivileged_icmp = true
            [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.runc]
              cgroup_writable = true
            [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia]
              container_annotations = ["nvidia.cdi.k8s.io/*"]
              privileged_without_host_devices = false
              runtime_engine = ""
              runtime_root = ""
              runtime_type = "io.containerd.runc.v2"
              [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia.options]
                BinaryName = "/var/nvidia/toolkit/nvidia-container-runtime"
            [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-cdi]
              container_annotations = ["nvidia.cdi.k8s.io/*"]
              privileged_without_host_devices = false
              runtime_engine = ""
              runtime_root = ""
              runtime_type = "io.containerd.runc.v2"
              [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-cdi.options]
                BinaryName = "/var/nvidia/toolkit/nvidia-container-runtime.cdi"
            [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-legacy]
              container_annotations = ["nvidia.cdi.k8s.io/*"]
              privileged_without_host_devices = false
              runtime_engine = ""
              runtime_root = ""
              runtime_type = "io.containerd.runc.v2"
              [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.nvidia-legacy.options]
                BinaryName = "/var/nvidia/toolkit/nvidia-container-runtime.legacy"

  # NFS mount options
  - |
    machine:
      files:
        - op: overwrite
          path: /etc/nfsmount.conf
          permissions: 0o644
          content: |
            [ NFSMount_Global_Options ]
            hard=True
            noatime=True
            nodiratime=True

controlPlane:
  patches:
    # Cluster configuration
    - |-
      cluster:
        allowSchedulingOnControlPlanes: true
        controllerManager:
          extraArgs:
            bind-address: 0.0.0.0
        coreDNS:
          disabled: true
        proxy:
          disabled: true
        scheduler:
          extraArgs:
            bind-address: 0.0.0.0

    # ETCD configuration
    - |-
      cluster:
        etcd:
          extraArgs:
            listen-metrics-urls: http://0.0.0.0:2381
          advertisedSubnets:
            - 10.1.1.0/24

    # api-server configuration
    - |-
      cluster:
        apiServer:
          extraArgs:
            enable-aggregator-routing: true
            feature-gates: UserNamespacesSupport=true,UserNamespacesPodSecurityStandards=true
            oidc-client-id: "${OIDC_CLIENT_ID}"
            oidc-issuer-url: "${OIDC_ISSUER_URL}"
            oidc-username-claim: email

    # Disable default API server admission plugins.
    - |-
      - op: remove
        path: /cluster/apiServer/admissionControl

    # Enable K8s Talos API Access
    - |-
      machine:
        features:
          kubernetesTalosAPIAccess:
            enabled: true
            allowedRoles:
              - os:admin
              - os:etcd:backup
            allowedKubernetesNamespaces:
              - talos-admin
